\documentclass[oneside, a4paper, DIV=10]{scrartcl}


% PACKAGES
\usepackage[english]{babel}
\usepackage{listings} % Required for insertion of code

% SETTINGS
\input{./docu/lstsettings}

% TITLE
\title{Exercise Sheet VIII}
\author{G\"unther Schindler, Klaus Naumann \& Christoph Klein}

% DOCUMENT
\begin{document}
\maketitle

% PART 1
%%%%%%%%
\section*{Reviews}
\begin{itemize}
    % FIRST REVIEW: MEMORY SENSITIVITY OF HPC APPLICATIONS
    \item
    The paper 'On the Effects of Memory Latency and Bandwidth on
    Supercomputer Application Performance' from Richard Murphy
    published in 2007 discusses the memory sensitivity of two types
    of high performance applications. There are traditional floating
    point based applications, which solves mostly physical problems,
    and emerging integer arithmetic based applications, which solve mostly
    graph problems.

    The author performed benchmarks for this two types of applications
    on a parallel machine simulator and measured the average instructions
    per clock cycle in dependency of the memory latency and bandwidth. Based
    on the results for the choosen benchmarks he says that both types of
    applications are rather latency than bandwidth sensitive. Furthermore
    the integer based applications were generally more memory 
    sensitive.

    To our mind this paper shows the divergence between HPC-architectures,
    which are optimized for traditional floating point operations, and the
    emerging amount of integer/tree based applications. But the author
    does not give constructive suggestions to solve this divergence. Furthermore
    one should keep in mind that the measurements were not done on a real HPC-system.
    This paper looks like an appeal at HPC-architects to consider the emerging
    amount of integer-based/memory-sensitive applications.

    % SECOND REVIEW: MEMORY ACCESS PATTERNS OF HPC APPLICATIONS
    \item
    The paper 'On the Memory Access Patterns of Supercomputer Applications:
    Benchmark Selection and Its Implications' from Murphy and Kogge published
    in July 2007 discusses the different demands of real HPC applications in 
    comparision to known HPC benchmarks.

    The authors compare mainly a common benchmark suite (integer/floating-point based)
    with real HPC applications (integer/floating-point based). They choose the
    following metrics for comparison:
    \begin{itemize}
        \item \emph{Temporal locality:} metric for data reusage
        \item \emph{Spatial locality:} amount of relatively cache hits
        \item \emph{Data intensiveness:} amount of unique accessed data
    \end{itemize}
    The authors analyzed this metrics system independently for real
    applications from Sandia National Laboratories and for common benchmark
    suites from the System Performance Evaluation Cooperative (SPEC). Based on
    the results the authors say that the SPEC benchmark suites differ enormously
    in the choosen metrics for the floating-point and integer based benchmarks from the
    real applications represented by Sandia benchmark suites. Especially the common
    LINPACK benchmark does not seem to represent the demands of real applications
    in any way. Even huge differences between Sandia and SPEC suites can be found
    by estimating the data intensiveness, where Sandia suites have a much
    higher data intensiveness than SPEC suites, which is crucial, as the number
    of unique accessed items can affect the memory performance more than the 
    store efficiency.

    To our mind this paper provides scientific arguments for what was already
    supposed: Many benchmarks do not represent the demands of a real application
    and should therefore be choosen wisely (i.e. unrealistic LINPACK).
\end{itemize}    

% PART 2
%%%%%%%%

% PART 3
%%%%%%%%
    
\end{document}
