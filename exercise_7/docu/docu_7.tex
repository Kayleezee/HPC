\documentclass[oneside, a4paper, DIV=10]{scrartcl}


% PACKAGES
\usepackage[english]{babel}
\usepackage{listings} % Required for insertion of code

% TITLE
\title{Exercise Sheet VII}
\author{G\"unther Schindler, Klaus Naumann \& Christoph Klein}

% DOCUMENT
\begin{document}
\maketitle

% PART 1
%%%%%%%%

\section*{Reviews}
\begin{itemize}
    % first review
    \item 
    The paper 'Roofline: An Insightful Visual Performance Model for Multicore
    Architectures' from Williams, Waterman and Patterson published in April 2009
    starts with outlining the importance and usefulness of a model, which provides
    performance guidelines for computational kernels on various computer
    architectures.

    They assert that memory bandwidth and the processor's maximal possible
    calculations per time are the main limiting performance factors. They
    visualize these performance limits in a coordinate system, with
    operations per second on the $y$- and operational intensity,  which is
    the amount of operations divided by the size of the fetched data
    (e.g. from memory to cache or from cache to processor), on the 
    $x$-axis.

    If a programmer determines the operational intensity of a kernel he can
    estimate the limiting performance factor in the described coordinate
    system. The advantage for the programmer is that he knows how to optimize
    efficiently his code.

    To our mind the Roofline-Model provides a handy guideline for how to
    determining bottlenecks and optimizing efficiently on various 
    computer architectures.

    % second review
    \item
    The paper 'LogP: Towards a Realistic Model of Parallel Computation' from
    Culler et al. published in July 1993 starts with the necessity of an
    intermediate complex model for portable algorithms on massively parallel
    processors. 
    
    To their mind the popular models (e.g. PRAM) are not applicable for real
    algorithms. Therefore they created the LogP Model, which is based on the BSP-Model
    from Valiant and has the following parameters:
    
    \begin{description}
        \item[$L$] an upper bound on latency for point to point communication
        \item[$o$] an overhead time for sending or recieving a message
        \item[$g$] time interval between consecutive sending or recieving
        \item[$P$] number of processors/memory modules
    \end{description}
    
    Furthermore they assume a finite capacity ($\lceil L/g \rceil$) of the
    network.

    With this assumptions the authors derive optimized algorithms like a
    broadcast, summation, FFT and LU decomposition. By matching the
    model to real machines the authors explain that different network
    types (Hypercube, Butterfly, 3D Torus, ...) differ in their average
    distance only by a factor of maximal four, hence other factors like
    send and recieve overhead dominate the network overhead.

    The model's name means the model for communication time as an arbitrary
    constant like $L = \log P$.

    To our mind the model provides an accurate framework to develop portable
    parallel algorithms, but because of it's generality it is not very accurate
    . For example we know that a preposted recieve in MPI is much faster
    than a postposted recieve, which contradicts to the constant overhead
    in the LogP model.
\end{itemize}
\section*{n-Body Problem - Partitioning / Communication Design}
	To perform an n-Body simulation every body needs data from all bodies in the
    simulation for calculation purposes. For the MPI implementation we need to 
    define a MPI Datatyp and a communication pattern.
    
    \subsection*{Datatype}
    As you can see in our sequential implementation we used a struct to store
    the body data. The needed data is:
    \begin{enumerate}
    	\item mass
        \item positions in x, y, z direction
        \item velocity in x, y, z direction
    \end{enumerate}
    For the parallel MPI implementation we add the force in each direction to
    the struct an define a MPI type.
    Depending on the amount of bodies and the memory each process could store
    the mass of every data, because compared the other data masses are static
    and always required for the calculation.
    
    \subsection*{Communication}
    We distribute the bodies between the processes to achieve symmetry for the
    n-Body simulation. As an advantage of this we can use the ring construct for
    communication. Every process \textit{P} only communicates with its neighbour
    processes \textit{P - 1} and \textit{P + 1}, where process \textit{P} gets
    data from \textit{P - 1} and sends data to \textit{P + 1} until all process
    have received all data for calculating the positions and forces.
    
    \subsection*{Maximize Overlap}
    To maximize the overlap between computation and communication we use 
    non-blocking send/recv. By using Isend and Irecv the processes can communicate
    while they calculate. 
    
\end{document}