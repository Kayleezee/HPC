%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{pgf} % Required to insert pgf plots from matplotlib
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
%\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ : \hmwkTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Perl, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\perlscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
%\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
%\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
%\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
%\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
%\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
%\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
%\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
%\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Übung\ \#2} % Assignment title
\newcommand{\hmwkDueDate}{Montag,\ 03.\ November\ 2014} % Due date
\newcommand{\hmwkClass}{Introduction to HPC} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkClassInstructor}{} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{Günther Schindler, Klaus Naumann, Christoph Klein} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Abgabe\ am\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	MPI Ring Communication
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}[MPI Ring Communication]

\begin{lstlisting}{c}

#include <mpi.h>
#include <stdio.h>
#include <unistd.h>

int main (int argc, char **argv) {

    int size, rank, m = 3000, signal = 666,i;
    double starttime, endtime;
    char hostname[50];
    MPI_Status status;

    MPI_Init (&argc, &argv);

    MPI_Comm_rank( MPI_COMM_WORLD, &rank);
    MPI_Comm_size( MPI_COMM_WORLD, &size);

    gethostname( hostname, 50);


    if (rank == 0 && size > 1) { /*at least two processes*/
        starttime = MPI_Wtime();
        for (i=0; i<m; i++) { /*main process sends the first m messages*/
            MPI_Send( &signal, 1, MPI_INT, 1, 0, MPI_COMM_WORLD); 
        }
        for (i=0; i<m; i++) { // main process waits for the last m messages
            MPI_Recv ( &signal, 1, MPI_INT, size-1, 0, MPI_COMM_WORLD, &status);
        }
        endtime = MPI_Wtime();
        double dt = endtime - starttime;
        double N = m*size;
        double average = dt/N;
        printf( "%f %f\n", dt, average);
    }
    else if (size > 1) { // at least two processes
        int source = rank - 1;
        for (i=0; i<m; i++) { // side processes wait for m messages
            MPI_Recv ( &signal, 1, MPI_INT, source, 0, MPI_COMM_WORLD, &status);
        }
   

        int destination = (rank + 1) % size;
        for (i=0; i<m; i++) { // side processes send m messages
            MPI_Send( &signal, 1, MPI_INT, destination, 0, MPI_COMM_WORLD);
        }
    }
    else printf ("No communication with only one process.");

    MPI_Finalize();
    return 0;

}\end{lstlisting}

\begin{figure}
    \begin{center}
        \input{./src/part_1/plot.pgf}
    \end{center}
    \caption{For every number of processes the total communication time was measured ten times.
        One point in the graph represents the average of the ten measurements. The average
        time per message was not measured in a direct way, rather it was calculated
        out of the total time. Every process sent 3000 messages to another process.}
\end{figure}    
\subsection*{Minimize average time per message for 12 processes}
If you test explicit this program, you achieve smallest average times per message for all
processes on one node. Due to the small amount of calculations the program's time
intensive parts are made up of communication between the processes. Therefore
the program is slower if it has to communicate via Ethernet. But if you think about a
more calculation intensive program, which has to make a ring communication it is
sensible to assign one process to one core. This means for the computers creek01-08, which
have octa cores, eight processes per computer. That means in case with 12 processes you should
assign the first 8 to one node and the last 4 to the next computing node.
In the measurements we got the times:
\begin{center}
\begin{tabular}{ll}
& average time per message [$\mu$s] \\
12 processes on one node & $\approx 1.5$ \\
8 processes and 4 processes on node & $\approx 4.5$ \\
\end{tabular}
\end{center}
\end{homeworkProblem}
\clearpage
%----------------------------------------------------------------------------------------
%	Barrier Synchronization
%----------------------------------------------------------------------------------------
\begin{homeworkProblem}[Barrier Synchronization]

\begin{lstlisting}{c}
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

void barrier(int *rank, int *size) {
	int i;
	int signal = 1;
	MPI_Status status;

		if (*(rank) == 0) {

			// root sends message to processes [1 ... *(size) - 1]
			for(i = 1; i < *(size); i++) {
				MPI_Send(&signal, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
				printf("%d: sent message to Process No. : %d\n", *(rank), i);
			}
			
			// root receives message from processes [1 ... *(size) - 1]
			for(i = 1; i < *(size); i++) {
				MPI_Recv(&signal, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);
				printf("%d: received message from Process No. : %d\n", *(rank), i);
			}
		
		} 
		else {
			// Process *(rank) receives message from root (rank 0)
			MPI_Recv(&signal, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
			printf("%d: received message from Process No. : %d\n", *(rank), status.MPI_SOURCE);			

			// Process *(rank) sends back message to root (rank 0)
			MPI_Send(&signal, 1, MPI_INT, status.MPI_SOURCE, 0, MPI_COMM_WORLD);
			printf("%d: sent message to Process No. : %d\n", *(rank), status.MPI_SOURCE);			
		}
}

int main(int argc, char **argv) {
	int i, size, rank;
	double t, starttime, endtime;
	int iterations = atoi(argv[1]);

	MPI_Init(&argc,&argv);		// Initialisation of MPI
	
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	MPI_Comm_size(MPI_COMM_WORLD, &size);
	
	for(i = 0; i < iterations; i++) {
		starttime = MPI_Wtime();
		//MPI_Barrier(MPI_COMM_WORLD);		
		barrier(&rank, &size);
		endtime = MPI_Wtime();
		t += (endtime - starttime);
	}

	MPI_Finalize();			// Deinitialisation of MPI
	printf("Time elapsed per iteration: %f\n", t/iterations);
	printf("Time elapsed: %f\n", t);
	return 0;
}
}
\end{lstlisting}

\begin{figure}
    \begin{center}
        %\input{./src/part_2/centralbarrier.png}
        \includegraphics[width=1\columnwidth]{centralbarrier}
    \end{center}
    \caption{Comparison between built-in MPI\_Barrier() and the implemented centralized Barrier(). 
    The measured time is the average time needed for a process to reach the barrier for one iteration. 
    Both barriers where tested for 2 up to 24 process in parallel and 100 iterations to achieve stable results.}
\end{figure}    
\subsection{Comparison between built-in MPI\_Barrier() and centralized barrier\ Barrier()}
The amount of time needed for a process to reach the barrier increases with increasing 
number of parallel processes. As you can see in the chart the gap between MPI\_Barrier() 
and the implemented centralized Barrier() gets bigger with rising amount of involved processes. 
For 24 processes MPI\_Barrier() performs 300 percent better than the centralized Barrier().
%\begin{center}

\end{homeworkProblem}
\clearpage
%----------------------------------------------------------------------------------------
%	Matrix multiply - sequential version
%----------------------------------------------------------------------------------------
\begin{homeworkProblem}[Matrix multiply - sequential version]
\subsection{Non-optimized Matrix}
As part of the second assignment in Intro HPC we have to implement a sequential,
non-opitmized, version of the matrix multiply. This implementation should multiply
two double precision floating point matrices. The matrices should be initialized by
random values. In addition, we have to measure the execution time for the operation.
\\ \\
For all operations with matrices, we introduced the structure sMatrix. This structure
contains one integer for rows, one integer for coloumns and a pointer to the address
of the matrix itself.
\begin{lstlisting}{c}
typedef struct sMatrix
{
  int iRow;          // for rows
  int iCol;          // for columns
  double** ppaMat;   // double[][]
} sMatrix;
\end{lstlisting}
To allocate memory, we implemented the function vAllocMatrix().
\begin{lstlisting}{c}
int vAllocMatrix(sMatrix *pM, int iRow, int iCol)
{ 
  int i=0, iRet=0;
  /* Init rows and cols in matrix structure */
  pM->iCol=iCol;
  pM->iRow=iRow;
  /* Alloc Mem for Rows-Pointer */
  pM->ppaMat = malloc(iRow * sizeof(double *));
  if(NULL == pM->ppaMat)
    iRet=1;
  /* Allocate Memory for Rows */
  for(i=0; i < iRow; i++)
  {
    pM->ppaMat[i] = malloc(iCol * sizeof(double *));
    if(NULL == pM->ppaMat[i])
      iRet=1;
  }
  return iRet;
}
\end{lstlisting}
The function vFreeMatrix() will free the allocated memory again.
\begin{lstlisting}{c}
void vFreeMatrix(sMatrix *pM)
{
  int i;
  /* free the Rows */
  for(i=0; i<pM->iRow; i++)
    free(pM->ppaMat[i]);
  /* free cols */
  free(pM->ppaMat);
}
\end{lstlisting}
The function vInitMatrix() initializes the matrix elements with random numbers, based
on the commited seed value.
\begin{lstlisting}{c}
void vInitMatrix(sMatrix *pM, int iSeed)
{
  int i,j;
  /* Initializes random number generator */
  srand((unsigned) iSeed);
  /* Fill the matrix-elements with random numbers */
  /* matrix[zeile][spalte] */
  for(i=0; i<pM->iRow; i++)
  {
    for(j=0; j<pM->iCol; j++)
      /* Generate numbers fronm 0 to 50 */
      pM->ppaMat[i][j]=(double)rand();
  }
}
\end{lstlisting}
The function vMatrixMultiply() multiplies two matrices (based on a naive algorithm).
\begin{lstlisting}{c}
void vMatrixMultiply(sMatrix *pMa, sMatrix *pMb, sMatrix *pMRes)
{
  int i,j,k;
  /* 
   * In order to multiply 2 matrices, one must have the same amount of rows that the 
   * other has columns.
   */
  if(pMa->iRow == pMb->iCol)
  {
    for(i=0; i<pMa->iRow; i++)
    {
      for(j=0; j<pMb->iCol; j++)
      {
        for(k=0; k<pMa->iCol; k++)
	  pMRes->ppaMat[i][j] += pMa->ppaMat[i][k] * pMb->ppaMat[k][j];
      }  
    }
  }
}
\end{lstlisting}
In order to measure the time we use the functions dstartMeasurement() and
dstopMeasurement().
\begin{lstlisting}{c}
/* Start time-measurement */
double dstartMeasurement(void)
{
  struct timeval tim;
  gettimeofday(&tim, NULL);
  return tim.tv_sec+(tim.tv_usec/1000000.0);
}
/* Stop time-measurement */
double dstopMeasurement(double dStartTime)
{
  struct timeval tim;
  gettimeofday(&tim, NULL);
  return (tim.tv_sec+(tim.tv_usec/1000000.0)) - dStartTime;
}
\end{lstlisting}
\subsection{Execution time}
The implemented program should be executed on one idle node of the creek servers.
\\ \\
The time measurement resulted in a value of ${\mathcal (t \approx 146.87s)}$.
\\The GFLOP/s (Giga Floating Point Operations Per Second) get calculated by the sum 
of floating point operations divided by the elapsed time. In order to determine the 
number of floating point operations we take a look at the algorithm.
\begin{lstlisting}{c}
if(pMa->iRow == pMb->iCol)
{
  for(i=0; i<pMa->iRow; i++)
  {
    for(j=0; j<pMb->iCol; j++)
    {
      for(k=0; k<pMa->iCol; k++)
        pMRes->ppaMat[i][j] += pMa->ppaMat[i][k] * pMb->ppaMat[k][j];
    }  
  }
}
\end{lstlisting}
The number of operations to be due to the single loop iterations:
\\${\mathcal O (i*j*k)}$ \\
For quadratic matrices ${\mathcal (i=j=k)}$ we get a cubic order:
\\${\mathcal O (n^3)}$. \\
For each loop iteration we get two floating point operations. This results:
\\${\mathcal (2*(2048^3)) = 1.717986918*10^{10} \approx 17.18 GFLOP}$ \\
And for the spezific time measurement we get:
\\${\mathcal (17.18GFLOP / 146.87s) \approx 116.97 MFLOP/s}$ .
\subsection{Gap between achieved and theoretical GFLOP/s}
The theoretical peak performance for the Intel Xeon E5-1620 CPU, used in the creek-Server,
is (according to Intels datasheet: http://download.intel.com/support/processors/xeon/sb/xeon\_E5-1600.pdf)
115.2 GFLOP/s. The huge gap between the theoretical and the achieved GFLOP/s is due to
size of the matrices. The matrices are too big to be handled in the Cash, thus they need
to be moved to the RAM. At this, the slowless to access the RAM and the slowless
of the RAM itself (compared to the Cash) is the bottleneck in this system.
\subsection{Optimized matrix multiplication}
In order to overcome the locality problem of this program, we use a technique to
improve the Cache performance. This technique is called cache blocking, or cache
tiling. Here, matrices that are too big to fit in the cache are broken up into smaller
pieces that will fit in the cache. Following algorithm shows our blocking matrix-multiplication.
\begin{lstlisting}{c}
int iTilledMatrixMultiply(sMatrix *pMa, sMatrix *pMb, sMatrix *pMRes, int iBlockSize)
{
int i,j,k, ii, jj, kk;
  for(ii=0; ii<pMa->iRow; ii+=iBlockSize)
  {
    for(jj=0; jj<pMb->iCol; jj+=iBlockSize)
    {
      for(kk=0; kk<pMa->iCol; kk+=iBlockSize)
      {
        for(i=ii; i<ii+iBlockSize; i++)
        {
          for(j=jj; j<jj+iBlockSize; j++)
          {
            for(k=kk; k<kk+iBlockSize; k++)
            pMRes->ppaMat[i][j] += pMa->ppaMat[i][k] * pMb->ppaMat[k][j];
          }
        } 
      }
    }
  }
 return 0;
}
\end{lstlisting}
The block size (iBlockSize) is a very important part in this implementatio. The bigger
the blocks are, the greater the reduction in memory traffic. But only up to a point.
Three blocks must be able to fit into cache at the same time. In our case, the cache
is able to store 10 MB. Each matrix element is double precision (8 Byte). So we can
calculate the theoretical optimized block size by:
\\$\sqrt[2]{10 MB / 3 / 8 B} \approx 645$ .
\\For our algorithm we need a number based on the power of two. The next smallest is in
this case 512. So, 512 is theoretical the best block size for the matrix-multiply on
this system.
\\In reality the time measurements lead to a different result. Here we get the best
result by a block size of 16.
\\For the block size of 16 we achieve 323.23 GFLOP/s.
\\
\\\\
\begin{tabular}{|c|c|}\hline
   Block Size & Time in Sec \\ \hline
   4 & 55.06 \\ \hline
   8 &  65.74 \\ \hline
   16 & 53.15 \\ \hline
   32 & 54.76 \\ \hline
   64 & 55.06 \\ \hline
   128 & 56.47 \\ \hline
   256 & 65.06 \\ \hline
   512 & 72.76 \\ \hline
   1024 & 85.46 \\ \hline
   2048 & 146.53 \\ \hline
\end{tabular}

\end{homeworkProblem}
\clearpage
\end{document}
